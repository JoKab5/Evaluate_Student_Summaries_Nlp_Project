{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CommonLit - Evaluate Student Summaries Dataset with TensorFlow Decision Forests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook walks you through how to train a baseline Random Forest model using TensorFlow Decision Forests on the **CommonLit - Evaluate Student Summaries** dataset made available for this competition.\n",
    "\n",
    "Roughly, the code will look as follows:\n",
    "\n",
    "```\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"project/dataset.csv\")\n",
    "tf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(dataset, label=\"my_label\")\n",
    "\n",
    "model = tfdf.keras.RandomForestModel()\n",
    "model.fit(tf_dataset)\n",
    "\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "Decision Forests are a family of tree-based models including Random Forests and Gradient Boosted Trees. They are the best place to start when working with tabular data, and will often outperform (or provide a strong baseline) before you begin experimenting with neural networks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import the libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:08.789060800Z",
     "start_time": "2023-10-11T00:10:08.742190800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (2.1.0+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\joelk\\anaconda3\\envs\\bowja\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:10.942939300Z",
     "start_time": "2023-10-11T00:10:08.789060800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import textstat\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:02.197526Z",
     "iopub.execute_input": "2023-09-24T14:17:02.198671Z",
     "iopub.status.idle": "2023-09-24T14:17:02.205092Z",
     "shell.execute_reply.started": "2023-09-24T14:17:02.198619Z",
     "shell.execute_reply": "2023-09-24T14:17:02.203602Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:19.608378Z",
     "start_time": "2023-10-11T00:10:10.942939300Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "# print(\"TensorFlow Decision Forests v\" + tfdf.__version__)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:02.367836Z",
     "iopub.execute_input": "2023-09-24T14:17:02.368174Z",
     "iopub.status.idle": "2023-09-24T14:17:02.374086Z",
     "shell.execute_reply.started": "2023-09-24T14:17:02.368148Z",
     "shell.execute_reply": "2023-09-24T14:17:02.372801Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:19.623983400Z",
     "start_time": "2023-10-11T00:10:19.608378Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow v2.14.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:19.671183500Z",
     "start_time": "2023-10-11T00:10:19.623983400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the prompt csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df_train_prompt = pd.read_csv('data/prompts_train.csv')\n",
    "print(\"Full prompt train dataset shape is {}\".format(df_train_prompt.shape))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:02.827662Z",
     "iopub.execute_input": "2023-09-24T14:17:02.828024Z",
     "iopub.status.idle": "2023-09-24T14:17:02.839125Z",
     "shell.execute_reply.started": "2023-09-24T14:17:02.827995Z",
     "shell.execute_reply": "2023-09-24T14:17:02.837877Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:19.671183500Z",
     "start_time": "2023-10-11T00:10:19.639785200Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prompt train dataset shape is (4, 4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is composed of 4 columns and 4 entries. We can see all 4 dimensions of our dataset by using the following code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df_train_prompt.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:03.158355Z",
     "iopub.execute_input": "2023-09-24T14:17:03.159252Z",
     "iopub.status.idle": "2023-09-24T14:17:03.171990Z",
     "shell.execute_reply.started": "2023-09-24T14:17:03.159200Z",
     "shell.execute_reply": "2023-09-24T14:17:03.170377Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:19.702343300Z",
     "start_time": "2023-10-11T00:10:19.671183500Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  prompt_id                                    prompt_question  \\\n0    39c16e  Summarize at least 3 elements of an ideal trag...   \n1    3b9047  In complete sentences, summarize the structure...   \n2    814d6b  Summarize how the Third Wave developed over su...   \n3    ebad26  Summarize the various ways the factory would u...   \n\n                prompt_title  \\\n0                 On Tragedy   \n1  Egyptian Social Structure   \n2             The Third Wave   \n3    Excerpt from The Jungle   \n\n                                         prompt_text  \n0  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n1  Egyptian society was structured like a pyramid...  \n2  Background \\r\\nThe Third Wave experiment took ...  \n3  With one member trimming beef in a cannery, an...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt_id</th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39c16e</td>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3b9047</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>814d6b</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ebad26</td>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>With one member trimming beef in a cannery, an...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the summaries csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df_train_summaries = pd.read_csv('data/summaries_train.csv')\n",
    "print(\"Full summaries train dataset shape is {}\".format(df_train_summaries.shape))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:03.485069Z",
     "iopub.execute_input": "2023-09-24T14:17:03.485535Z",
     "iopub.status.idle": "2023-09-24T14:17:03.534377Z",
     "shell.execute_reply.started": "2023-09-24T14:17:03.485500Z",
     "shell.execute_reply": "2023-09-24T14:17:03.533019Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:19.765354900Z",
     "start_time": "2023-10-11T00:10:19.686710400Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full summaries train dataset shape is (7165, 5)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is composed of 5 columns and 7165 entries. We can see all 5 dimensions of our dataset by printing out the first 5 entries using the following code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df_train_summaries.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:03.821404Z",
     "iopub.execute_input": "2023-09-24T14:17:03.822815Z",
     "iopub.status.idle": "2023-09-24T14:17:03.833340Z",
     "shell.execute_reply.started": "2023-09-24T14:17:03.822761Z",
     "shell.execute_reply": "2023-09-24T14:17:03.832329Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:19.812333100Z",
     "start_time": "2023-10-11T00:10:19.733558300Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "     student_id prompt_id                                               text  \\\n0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n\n    content   wording  \n0  0.205683  0.380538  \n1 -0.548304  0.506755  \n2  3.128928  4.231226  \n3 -0.210614 -0.471415  \n4  3.272894  3.219757  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000e8c3c7ddb</td>\n      <td>814d6b</td>\n      <td>The third wave was an experimentto see how peo...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0020ae56ffbf</td>\n      <td>ebad26</td>\n      <td>They would rub it up with soda to make the sme...</td>\n      <td>-0.548304</td>\n      <td>0.506755</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>004e978e639e</td>\n      <td>3b9047</td>\n      <td>In Egypt, there were many occupations and soci...</td>\n      <td>3.128928</td>\n      <td>4.231226</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>005ab0199905</td>\n      <td>3b9047</td>\n      <td>The highest class was Pharaohs these people we...</td>\n      <td>-0.210614</td>\n      <td>-0.471415</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0070c9e7af47</td>\n      <td>814d6b</td>\n      <td>The Third Wave developed  rapidly because the ...</td>\n      <td>3.272894</td>\n      <td>3.219757</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "combi = df_train_prompt.merge(df_train_summaries, how=\"left\", on=\"prompt_id\")\n",
    "# saving the dataframe\n",
    "combi.to_csv('merge_train.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:20.158336900Z",
     "start_time": "2023-10-11T00:10:19.749711200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to do next on sunday:\n",
    "1. pos: part of speech\n",
    "2. Jaccard Similarity\n",
    "3. Average Word Length: Calculate the average word length in the summary and the prompt.\n",
    "4. Average Sentence Length: Compute the average sentence length in the summary and the prompt.\n",
    "5. Keyword Matching : Identify and count specific keywords or phrases related to the prompt that appear in the summary.\n",
    "6. NER : Identify and count named entities in both the prompt and the summary.\n",
    "7. Apply topic modeling techniques (e.g., LDA) to identify and compare the main topics in the prompt and the summary.\n",
    "8. Semantic similarity: Compute semantic similarity scores (e.g., Word Mover's Distance) between the prompt and the summary.\n",
    "9. Use readability metrics (e.g., Flesch-Kincaid, Gunning Fog Index) to measure the readability of both the prompt and the summary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "twd = TreebankWordDetokenizer()\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "spacy_ner_model = spacy.load('en_core_web_sm',)\n",
    "speller = Speller(lang='en')\n",
    "spellchecker = SpellChecker() \n",
    "\n",
    "def word_overlap_count(row):\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        if STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "def ngrams(token, n):\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[token[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "def ngram_co_occurrence(row, n) -> int:\n",
    "    # Tokenize the original text and summary into words\n",
    "    original_tokens = row['prompt_tokens']\n",
    "    summary_tokens = row['summary_tokens']\n",
    "\n",
    "    # Generate n-grams for the original text and summary\n",
    "    original_ngrams = set(ngrams(original_tokens, n))\n",
    "    summary_ngrams = set(ngrams(summary_tokens, n))\n",
    "\n",
    "    # Calculate the number of common n-grams\n",
    "    common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "    return len(common_ngrams)\n",
    "    \n",
    "def ner_overlap_count(row, mode):\n",
    "    model = spacy_ner_model\n",
    "    def clean_ners(ner_list):\n",
    "        return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "    prompt = model(row['prompt_text'])\n",
    "    summary = model(row['text'])\n",
    "\n",
    "    if \"spacy\" in str(model):\n",
    "        prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "        summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "    elif \"stanza\" in str(model):\n",
    "        prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "        summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "    else:\n",
    "        raise Exception(\"Model not supported\")\n",
    "\n",
    "    prompt_ner = clean_ners(prompt_ner)\n",
    "    summary_ner = clean_ners(summary_ner)\n",
    "    \n",
    "    intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "    ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        return ner_dict\n",
    "    elif mode == \"test\":\n",
    "        return {key: ner_dict.get(key) for key in ner_keys}\n",
    "\n",
    "\n",
    "def quotes_count(row):\n",
    "    summary = row['text']\n",
    "    text = row['prompt_text']\n",
    "    quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "    if len(quotes_from_summary)>0:\n",
    "        return [quote in text for quote in quotes_from_summary].count(True)\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def spelling(text):\n",
    "\n",
    "    wordlist= text.split()\n",
    "    amount_miss = len(list(spellchecker.unknown(wordlist)))\n",
    "\n",
    "    return amount_miss\n",
    "    \n",
    "def add_spelling_dictionary(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n",
    "    spellchecker.word_frequency.load_words(tokens)\n",
    "    speller.nlp_data.update({token:1000 for token in tokens})\n",
    "    \n",
    "    \n",
    "####### new method\n",
    "def count_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def count_stopwords(text: str) -> int:\n",
    "    stopword_list = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    stopwords_count = sum(1 for word in words if word.lower() in stopword_list)\n",
    "    return stopwords_count\n",
    "\n",
    "# Count the punctuations in the text.\n",
    "# punctuation_set -> !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "def count_punctuation(text: str) -> int:\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    punctuation_count = sum(1 for char in text if char in punctuation_set)\n",
    "    return punctuation_count\n",
    "\n",
    "# Count the digits in the text.\n",
    "def count_numbers(text: str) -> int:\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    numbers_count = len(numbers)\n",
    "    return numbers_count\n",
    "\n",
    "# Function to extract POS tags for a given text\n",
    "def extract_pos_tags(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    pos_tag_list = [tag for _, tag in pos_tags]\n",
    "    return pos_tag_list\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Define relevant parts of speech for keywords (e.g., nouns, adjectives)\n",
    "    relevant_pos = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']\n",
    "    \n",
    "    keywords = [word for word, pos in pos_tags if pos in relevant_pos]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Function to count keyword matches in the summary\n",
    "def count_keyword_matches(prompt_keywords, summary):\n",
    "    summary = summary.lower()  # Convert summary to lowercase for case-insensitive matching\n",
    "    count = 0\n",
    "    for keyword in prompt_keywords:\n",
    "        if keyword.lower() in summary:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# Function to calculate average sentence length\n",
    "def calculate_average_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]  # Calculate word count for each sentence\n",
    "    if len(sentence_lengths) > 0:\n",
    "        return sum(sentence_lengths) / len(sentence_lengths)  # Calculate average sentence length\n",
    "    else:\n",
    "        return 0  # Return 0 if there are no sentences\n",
    "\n",
    "# Function to calculate average word length\n",
    "def calculate_average_word_length(text):\n",
    "    words = text.split()  # Split text into words\n",
    "    word_lengths = [len(word) for word in words]  # Calculate the length of each word\n",
    "    if len(word_lengths) > 0:\n",
    "        return sum(word_lengths) / len(word_lengths)  # Calculate average word length\n",
    "    else:\n",
    "        return 0  # Return 0 if there are no words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function applies all the above preprocessing functions on a text feature  \n",
    "\n",
    "def run(prompts: pd.DataFrame, summaries:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # before merge preprocess\n",
    "    prompts[\"original_prompt_len\"] = prompts[\"prompt_text\"].apply(lambda x: len(x))\n",
    "    \n",
    "    prompts['prompt_sentenceCount'] = prompts['prompt_text'].apply(lambda x:count_sentences(x))\n",
    "    \n",
    "    \n",
    "    prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(lambda x: len(word_tokenize(x)))\n",
    "    \n",
    "    prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(lambda x: word_tokenize(x))\n",
    "    \n",
    "    prompts[\"prompt_word_cnt\"] = prompts[\"prompt_text\"].apply(lambda x:len(x. split(' ')))\n",
    "    \n",
    "    prompts[\"prompt_stpword_cnt\"] = prompts[\"prompt_text\"].apply(lambda x: count_stopwords(x))\n",
    "    \n",
    "    #prompts[\"prompt_punct_cnt\"] = prompts[\"prompt_text\"].apply(lambda x: count_punctuation(x))\n",
    "    \n",
    "    #prompts[\"prompt_num_cnt\"] = prompts[\"prompt_text\"].apply(lambda x: count_numbers(x))\n",
    "    \n",
    "    # Add prompt tokens into spelling checker dictionary\n",
    "    prompts[\"prompt_tokens\"].apply(lambda x: add_spelling_dictionary(x))\n",
    "    \n",
    "    \n",
    "    # Add POS features for prompt_question, prompt_title, and prompt_text\n",
    "    prompts['question_pos_tags'] = prompts['prompt_question'].apply(extract_pos_tags)\n",
    "    prompts['prompt_text_pos_tags'] = prompts['prompt_text'].apply(extract_pos_tags)\n",
    "    \n",
    "    # Example: Count the occurrences of specific POS tags (e.g., nouns, verbs)\n",
    "    prompts['noun_count'] = prompts['prompt_text_pos_tags'].apply(lambda x: x.count('NN'))\n",
    "    prompts['verb_count'] = prompts['prompt_text_pos_tags'].apply(lambda x: x.count('VB'))\n",
    "    \n",
    "    # Example: TextBlob sentiment analysis\n",
    "    prompts['prompt_textblob_polarity'] = prompts['prompt_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    prompts['prompt_textblob_subjectivity'] = prompts['prompt_text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    \n",
    "    # Automatically extract keywords from the prompt\n",
    "    prompts['prompt_keywords'] = prompts['prompt_text'].apply(extract_keywords)\n",
    "    \n",
    "    \n",
    "    # Calculate average sentence length for the prompt_text and student_summary columns\n",
    "    prompts['avg_sentence_length_prompt'] = prompts['prompt_text'].apply(calculate_average_sentence_length)\n",
    "    summaries['avg_sentence_length_summary'] = summaries['text'].apply(calculate_average_sentence_length)\n",
    "    \n",
    "    # Calculate average word length for the prompt_text and student_summary columns\n",
    "    prompts['avg_word_length_prompt'] = prompts['prompt_text'].apply(calculate_average_word_length)\n",
    "    summaries['avg_word_length_summary'] = summaries['text'].apply(calculate_average_word_length)\n",
    "\n",
    "    \n",
    "    # Add POS features for prompt_question, prompt_title, and prompt_text\n",
    "    summaries['text_pos_tags'] = summaries['text'].apply(extract_pos_tags)\n",
    "    \n",
    "    \n",
    "    # Example: Count the occurrences of specific POS tags (e.g., nouns, verbs)\n",
    "    summaries['text_noun_count'] = summaries['text_pos_tags'].apply(lambda x: x.count('NN'))\n",
    "    summaries['text_verb_count'] = summaries['text_pos_tags'].apply(lambda x: x.count('VB'))\n",
    "    \n",
    "    summaries['textblob_polarity'] = summaries['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    summaries['textblob_subjectivity'] = summaries['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    \n",
    "    summaries[\"original_summary_len\"] = summaries[\"text\"].apply(lambda x: len(x))\n",
    "    \n",
    "    summaries['summary_sentenceCount'] = summaries['text'].apply(lambda x:count_sentences(x))\n",
    "\n",
    "    summaries[\"summary_length\"] = summaries[\"text\"].apply(lambda x: len(word_tokenize(x)))\n",
    "    \n",
    "    summaries[\"summary_tokens\"] = summaries[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "    \n",
    "    summaries[\"summary_word_cnt\"] = summaries[\"text\"].apply(lambda x:len(x.split(' ')))\n",
    "    \n",
    "    #summaries[\"summary_stpword_cnt\"] = summaries[\"text\"].apply(lambda x: count_stopwords(x))\n",
    "    \n",
    "    #summaries[\"summary_punct_cnt\"] = summaries[\"text\"].apply(lambda x: count_punctuation(x))\n",
    "    \n",
    "    #summaries[\"summary_num_cnt\"] = summaries[\"text\"].apply(lambda x: count_numbers(x))\n",
    "    \n",
    "    \n",
    "    #from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    # fix misspelling\n",
    "    summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(lambda x: speller(x))\n",
    "    \n",
    "    # count misspelling\n",
    "    summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(spelling)\n",
    "    \n",
    "    # merge prompts and summaries\n",
    "    input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "    # after merge preprocess\n",
    "    # input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
    "    \n",
    "    input_df['word_overlap_count'] = input_df.progress_apply(word_overlap_count, axis=1)\n",
    "    input_df['bigram_overlap_count'] = input_df.progress_apply(ngram_co_occurrence,args=(2,), axis=1)\n",
    "    input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n",
    "    \n",
    "    input_df['trigram_overlap_count'] = input_df.progress_apply(ngram_co_occurrence, args=(3,), axis=1)\n",
    "    input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n",
    "    \n",
    "    input_df['quotes_count'] = input_df.progress_apply(quotes_count, axis=1)\n",
    "    \n",
    "    return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:10:21.357697700Z",
     "start_time": "2023-10-11T00:10:20.173949400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [04:21<00:00, 27.41it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 12914.34it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 16007.98it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 7537.33it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 6739.01it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 152467.91it/s]\n"
     ]
    }
   ],
   "source": [
    "result = run(df_train_prompt, df_train_summaries)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:22:15.426832100Z",
     "start_time": "2023-10-11T00:17:27.009335800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7165 entries, 0 to 7164\n",
      "Data columns (total 41 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   student_id                    7165 non-null   object \n",
      " 1   prompt_id                     7165 non-null   object \n",
      " 2   text                          7165 non-null   object \n",
      " 3   content                       7165 non-null   float64\n",
      " 4   wording                       7165 non-null   float64\n",
      " 5   avg_sentence_length_summary   7165 non-null   float64\n",
      " 6   avg_word_length_summary       7165 non-null   float64\n",
      " 7   text_pos_tags                 7165 non-null   object \n",
      " 8   text_noun_count               7165 non-null   int64  \n",
      " 9   text_verb_count               7165 non-null   int64  \n",
      " 10  textblob_polarity             7165 non-null   float64\n",
      " 11  textblob_subjectivity         7165 non-null   float64\n",
      " 12  original_summary_len          7165 non-null   int64  \n",
      " 13  summary_sentenceCount         7165 non-null   int64  \n",
      " 14  summary_length                7165 non-null   int64  \n",
      " 15  summary_word_cnt              7165 non-null   int64  \n",
      " 16  fixed_summary_text            7165 non-null   object \n",
      " 17  splling_err_num               7165 non-null   int64  \n",
      " 18  prompt_question               7165 non-null   object \n",
      " 19  prompt_title                  7165 non-null   object \n",
      " 20  prompt_text                   7165 non-null   object \n",
      " 21  original_prompt_len           7165 non-null   int64  \n",
      " 22  prompt_sentenceCount          7165 non-null   int64  \n",
      " 23  prompt_length                 7165 non-null   int64  \n",
      " 24  prompt_word_cnt               7165 non-null   int64  \n",
      " 25  prompt_stpword_cnt            7165 non-null   int64  \n",
      " 26  question_pos_tags             7165 non-null   object \n",
      " 27  prompt_text_pos_tags          7165 non-null   object \n",
      " 28  noun_count                    7165 non-null   int64  \n",
      " 29  verb_count                    7165 non-null   int64  \n",
      " 30  prompt_textblob_polarity      7165 non-null   float64\n",
      " 31  prompt_textblob_subjectivity  7165 non-null   float64\n",
      " 32  prompt_keywords               7165 non-null   object \n",
      " 33  avg_sentence_length_prompt    7165 non-null   float64\n",
      " 34  avg_word_length_prompt        7165 non-null   float64\n",
      " 35  word_overlap_count            7165 non-null   int64  \n",
      " 36  bigram_overlap_count          7165 non-null   int64  \n",
      " 37  bigram_overlap_ratio          7165 non-null   float64\n",
      " 38  trigram_overlap_count         7165 non-null   int64  \n",
      " 39  trigram_overlap_ratio         7165 non-null   float64\n",
      " 40  quotes_count                  7165 non-null   int64  \n",
      "dtypes: float64(12), int64(18), object(11)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "result.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:14:48.629922300Z",
     "start_time": "2023-10-11T00:14:48.598565800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Full summaries train dataset shape is {}\".format(result.shape))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:06.091516Z",
     "iopub.execute_input": "2023-09-24T14:17:06.092098Z",
     "iopub.status.idle": "2023-09-24T14:17:07.272159Z",
     "shell.execute_reply.started": "2023-09-24T14:17:06.092060Z",
     "shell.execute_reply": "2023-09-24T14:17:07.271227Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:14:48.661074400Z",
     "start_time": "2023-10-11T00:14:48.629922300Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full summaries train dataset shape is (7165, 41)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "result['prompt_text_pos_tags'].head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:07.273747Z",
     "iopub.execute_input": "2023-09-24T14:17:07.274041Z",
     "iopub.status.idle": "2023-09-24T14:17:07.290490Z",
     "shell.execute_reply.started": "2023-09-24T14:17:07.274015Z",
     "shell.execute_reply": "2023-09-24T14:17:07.289009Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:14:48.667274300Z",
     "start_time": "2023-10-11T00:14:48.645484400Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [IN, DT, NNP, NNP, NN, VBD, NN, IN, NNP, NNP, ...\n1    [IN, CD, NN, VBG, NN, IN, DT, NN, ,, CC, DT, N...\n2    [JJ, NN, VBD, VBN, IN, DT, NN, ., IN, DT, NN, ...\n3    [JJ, NN, VBD, VBN, IN, DT, NN, ., IN, DT, NN, ...\n4    [IN, DT, NNP, NNP, NN, VBD, NN, IN, NNP, NNP, ...\nName: prompt_text_pos_tags, dtype: object"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "result['keyword_count'] = result.apply(lambda row: count_keyword_matches(row['prompt_keywords'], row['text']), axis=1)\n",
    "\n",
    "\n",
    "result['text_pos_len'] = result['text_pos_tags'].apply(lambda x: len(x))\n",
    "result['prompt_pos_len'] = result['prompt_text_pos_tags'].apply(lambda x: len(x))\n",
    "\n",
    "result['text_adj_count'] = result['text_pos_tags'].apply(lambda x: x.count('JJ'))\n",
    "result['prompt_adj_count'] = result['prompt_text_pos_tags'].apply(lambda x: x.count('JJ'))\n",
    "\n",
    "result['text_adj_ratio'] = result['text_adj_count'] / result['text_pos_len']\n",
    "result['prompt_adj_ration'] = result['prompt_adj_count'] / result['prompt_pos_len']\n",
    "\n",
    "result['text_verb_ratio'] = result['text_verb_count'] / result['text_pos_len']\n",
    "result['prompt_verb_ration'] = result['verb_count'] / result['prompt_pos_len']\n",
    "\n",
    "result['text_noun_ratio'] = result['text_noun_count'] / result['text_pos_len']\n",
    "result['prompt_noun_ration'] = result['noun_count'] / result['prompt_pos_len']\n",
    "\n",
    "# Calculate readability metrics for prompt and summary\n",
    "result['flesch_kincaid_prompt'] = result['prompt_question'].apply(textstat.flesch_kincaid_grade)\n",
    "result['flesch_kincaid_summary'] = result['text'].apply(textstat.flesch_kincaid_grade)\n",
    "result['gunning_fog_prompt'] = result['prompt_question'].apply(textstat.gunning_fog)\n",
    "result['gunning_fog_summary'] = result['text'].apply(textstat.gunning_fog)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:24:24.201002500Z",
     "start_time": "2023-10-11T00:24:22.264035300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Define a function to calculate Jaccard similarity\n",
    "def jaccard_similarity(str1, str2):\n",
    "    # Tokenize the strings and convert them to sets\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Calculate Jaccard similarity between prompt_question and text\n",
    "result['jaccard_similarity'] = result.apply(lambda row: jaccard_similarity(row['prompt_question'], row['text']), axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:24:25.813169900Z",
     "start_time": "2023-10-11T00:24:25.666348100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "result['combined_text'] = result['prompt_text'] + ' ' + result['text']\n",
    "\n",
    "# Vectorize the text data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.85, max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(result['combined_text'])\n",
    "\n",
    "# Apply Latent Dirichlet Allocation (LDA) for topic modeling\n",
    "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Get the topics for the prompt_text\n",
    "prompt_topics = lda.transform(vectorizer.transform(result['prompt_text']))\n",
    "\n",
    "# Get the topics for the student_summary\n",
    "summary_topics = lda.transform(vectorizer.transform(result['text']))\n",
    "\n",
    "# Assign the topics to DataFrame\n",
    "result['prompt_topics'] = prompt_topics.tolist()\n",
    "result['summary_topics'] = summary_topics.tolist()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:24:42.129754400Z",
     "start_time": "2023-10-11T00:24:28.472142100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      prompt_question  \\\n0   Summarize how the Third Wave developed over su...   \n1   Summarize the various ways the factory would u...   \n2   In complete sentences, summarize the structure...   \n3   In complete sentences, summarize the structure...   \n4   Summarize how the Third Wave developed over su...   \n5   Summarize the various ways the factory would u...   \n6   In complete sentences, summarize the structure...   \n7   Summarize the various ways the factory would u...   \n8   Summarize at least 3 elements of an ideal trag...   \n9   Summarize at least 3 elements of an ideal trag...   \n10  In complete sentences, summarize the structure...   \n11  Summarize the various ways the factory would u...   \n12  In complete sentences, summarize the structure...   \n13  Summarize at least 3 elements of an ideal trag...   \n14  Summarize how the Third Wave developed over su...   \n15  Summarize the various ways the factory would u...   \n16  In complete sentences, summarize the structure...   \n17  Summarize the various ways the factory would u...   \n18  Summarize how the Third Wave developed over su...   \n19  Summarize at least 3 elements of an ideal trag...   \n20  Summarize how the Third Wave developed over su...   \n21  Summarize at least 3 elements of an ideal trag...   \n22  Summarize at least 3 elements of an ideal trag...   \n23  Summarize the various ways the factory would u...   \n24  In complete sentences, summarize the structure...   \n25  Summarize the various ways the factory would u...   \n26  In complete sentences, summarize the structure...   \n27  In complete sentences, summarize the structure...   \n28  Summarize at least 3 elements of an ideal trag...   \n29  In complete sentences, summarize the structure...   \n\n                 prompt_title  \\\n0              The Third Wave   \n1     Excerpt from The Jungle   \n2   Egyptian Social Structure   \n3   Egyptian Social Structure   \n4              The Third Wave   \n5     Excerpt from The Jungle   \n6   Egyptian Social Structure   \n7     Excerpt from The Jungle   \n8                  On Tragedy   \n9                  On Tragedy   \n10  Egyptian Social Structure   \n11    Excerpt from The Jungle   \n12  Egyptian Social Structure   \n13                 On Tragedy   \n14             The Third Wave   \n15    Excerpt from The Jungle   \n16  Egyptian Social Structure   \n17    Excerpt from The Jungle   \n18             The Third Wave   \n19                 On Tragedy   \n20             The Third Wave   \n21                 On Tragedy   \n22                 On Tragedy   \n23    Excerpt from The Jungle   \n24  Egyptian Social Structure   \n25    Excerpt from The Jungle   \n26  Egyptian Social Structure   \n27  Egyptian Social Structure   \n28                 On Tragedy   \n29  Egyptian Social Structure   \n\n                                        prompt_topics  \\\n0   [0.9976707222247121, 0.0011622064773270835, 0....   \n1   [0.0009218235174879116, 0.9981522727362265, 0....   \n2   [0.0011688524885937544, 0.001174645211872204, ...   \n3   [0.0011688524885937544, 0.001174645211872204, ...   \n4   [0.9976707222247121, 0.0011622064773270835, 0....   \n5   [0.0009218235174879116, 0.9981522727362265, 0....   \n6   [0.0011688524885937544, 0.001174645211872204, ...   \n7   [0.0009218235174879116, 0.9981522727362265, 0....   \n8   [0.001415458355819997, 0.001409109992094977, 0...   \n9   [0.001415458355819997, 0.001409109992094977, 0...   \n10  [0.0011688524885937544, 0.001174645211872204, ...   \n11  [0.0009218235174879116, 0.9981522727362265, 0....   \n12  [0.0011688524885937544, 0.001174645211872204, ...   \n13  [0.001415458355819997, 0.001409109992094977, 0...   \n14  [0.9976707222247121, 0.0011622064773270835, 0....   \n15  [0.0009218235174879116, 0.9981522727362265, 0....   \n16  [0.0011688524885937544, 0.001174645211872204, ...   \n17  [0.0009218235174879116, 0.9981522727362265, 0....   \n18  [0.9976707222247121, 0.0011622064773270835, 0....   \n19  [0.001415458355819997, 0.001409109992094977, 0...   \n20  [0.9976707222247121, 0.0011622064773270835, 0....   \n21  [0.001415458355819997, 0.001409109992094977, 0...   \n22  [0.001415458355819997, 0.001409109992094977, 0...   \n23  [0.0009218235174879116, 0.9981522727362265, 0....   \n24  [0.0011688524885937544, 0.001174645211872204, ...   \n25  [0.0009218235174879116, 0.9981522727362265, 0....   \n26  [0.0011688524885937544, 0.001174645211872204, ...   \n27  [0.0011688524885937544, 0.001174645211872204, ...   \n28  [0.001415458355819997, 0.001409109992094977, 0...   \n29  [0.0011688524885937544, 0.001174645211872204, ...   \n\n                                       summary_topics  prompt_topic_1  \\\n0   [0.7166481316313827, 0.015401043676678662, 0.2...        0.997671   \n1   [0.02161244422911558, 0.9570443049783564, 0.02...        0.000922   \n2   [0.06296437014671895, 0.04736791369332212, 0.8...        0.001169   \n3   [0.08841918467068985, 0.028057187596373157, 0....        0.001169   \n4   [0.7820817880146029, 0.06862669389643532, 0.14...        0.997671   \n5   [0.021058089327376895, 0.8931636755248883, 0.0...        0.000922   \n6   [0.012857424035880197, 0.049620897662872476, 0...        0.001169   \n7   [0.019859350988571122, 0.8760290135922674, 0.1...        0.000922   \n8   [0.017046221848662425, 0.015254946699526208, 0...        0.001415   \n9   [0.030917140205545455, 0.030766154069026847, 0...        0.001415   \n10  [0.10413216230659808, 0.08548414874560073, 0.8...        0.001169   \n11  [0.00960348995787378, 0.9223250371009079, 0.06...        0.000922   \n12  [0.05880257190189563, 0.01195579734035659, 0.9...        0.001169   \n13  [0.013656152898532517, 0.013554892922842936, 0...        0.001415   \n14  [0.9732359394914631, 0.013369480667734494, 0.0...        0.997671   \n15  [0.020847029319461675, 0.9583195927092473, 0.0...        0.000922   \n16  [0.02566758798731632, 0.11389609959111871, 0.8...        0.001169   \n17  [0.005677476709355365, 0.985942421661169, 0.00...        0.000922   \n18  [0.9140649906555004, 0.04637972020741349, 0.03...        0.997671   \n19  [0.03733120591612821, 0.03126621596330079, 0.9...        0.001415   \n20  [0.9343074736777033, 0.031160849619190058, 0.0...        0.997671   \n21  [0.020515611460293227, 0.021243669995712523, 0...        0.001415   \n22  [0.031112080025803332, 0.030908036159298102, 0...        0.001415   \n23  [0.003753383273219095, 0.9924677984621932, 0.0...        0.000922   \n24  [0.03320876254696947, 0.009689440985234136, 0....        0.001169   \n25  [0.01149989043393082, 0.9770042522121551, 0.01...        0.000922   \n26  [0.004978415178569605, 0.005081976104355061, 0...        0.001169   \n27  [0.02879489001922393, 0.09745626123637435, 0.8...        0.001169   \n28  [0.03214614841115621, 0.008345504154948266, 0....        0.001415   \n29  [0.011851729014614266, 0.02987654446203027, 0....        0.001169   \n\n    prompt_topic_2  prompt_topic_3  summary_topic_1  summary_topic_2  \\\n0         0.001162        0.001167         0.716648         0.015401   \n1         0.998152        0.000926         0.021612         0.957044   \n2         0.001175        0.997657         0.062964         0.047368   \n3         0.001175        0.997657         0.088419         0.028057   \n4         0.001162        0.001167         0.782082         0.068627   \n5         0.998152        0.000926         0.021058         0.893164   \n6         0.001175        0.997657         0.012857         0.049621   \n7         0.998152        0.000926         0.019859         0.876029   \n8         0.001409        0.997175         0.017046         0.015255   \n9         0.001409        0.997175         0.030917         0.030766   \n10        0.001175        0.997657         0.104132         0.085484   \n11        0.998152        0.000926         0.009603         0.922325   \n12        0.001175        0.997657         0.058803         0.011956   \n13        0.001409        0.997175         0.013656         0.013555   \n14        0.001162        0.001167         0.973236         0.013369   \n15        0.998152        0.000926         0.020847         0.958320   \n16        0.001175        0.997657         0.025668         0.113896   \n17        0.998152        0.000926         0.005677         0.985942   \n18        0.001162        0.001167         0.914065         0.046380   \n19        0.001409        0.997175         0.037331         0.031266   \n20        0.001162        0.001167         0.934307         0.031161   \n21        0.001409        0.997175         0.020516         0.021244   \n22        0.001409        0.997175         0.031112         0.030908   \n23        0.998152        0.000926         0.003753         0.992468   \n24        0.001175        0.997657         0.033209         0.009689   \n25        0.998152        0.000926         0.011500         0.977004   \n26        0.001175        0.997657         0.004978         0.005082   \n27        0.001175        0.997657         0.028795         0.097456   \n28        0.001409        0.997175         0.032146         0.008346   \n29        0.001175        0.997657         0.011852         0.029877   \n\n    summary_topic_3  \n0          0.267951  \n1          0.021343  \n2          0.889668  \n3          0.883524  \n4          0.149292  \n5          0.085778  \n6          0.937522  \n7          0.104112  \n8          0.967699  \n9          0.938317  \n10         0.810384  \n11         0.068071  \n12         0.929242  \n13         0.972789  \n14         0.013395  \n15         0.020833  \n16         0.860436  \n17         0.008380  \n18         0.039555  \n19         0.931403  \n20         0.034532  \n21         0.958241  \n22         0.937980  \n23         0.003779  \n24         0.957102  \n25         0.011496  \n26         0.989940  \n27         0.873749  \n28         0.959508  \n29         0.958272  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_topics</th>\n      <th>summary_topics</th>\n      <th>prompt_topic_1</th>\n      <th>prompt_topic_2</th>\n      <th>prompt_topic_3</th>\n      <th>summary_topic_1</th>\n      <th>summary_topic_2</th>\n      <th>summary_topic_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>[0.9976707222247121, 0.0011622064773270835, 0....</td>\n      <td>[0.7166481316313827, 0.015401043676678662, 0.2...</td>\n      <td>0.997671</td>\n      <td>0.001162</td>\n      <td>0.001167</td>\n      <td>0.716648</td>\n      <td>0.015401</td>\n      <td>0.267951</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>[0.0009218235174879116, 0.9981522727362265, 0....</td>\n      <td>[0.02161244422911558, 0.9570443049783564, 0.02...</td>\n      <td>0.000922</td>\n      <td>0.998152</td>\n      <td>0.000926</td>\n      <td>0.021612</td>\n      <td>0.957044</td>\n      <td>0.021343</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.06296437014671895, 0.04736791369332212, 0.8...</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.062964</td>\n      <td>0.047368</td>\n      <td>0.889668</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.08841918467068985, 0.028057187596373157, 0....</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.088419</td>\n      <td>0.028057</td>\n      <td>0.883524</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>[0.9976707222247121, 0.0011622064773270835, 0....</td>\n      <td>[0.7820817880146029, 0.06862669389643532, 0.14...</td>\n      <td>0.997671</td>\n      <td>0.001162</td>\n      <td>0.001167</td>\n      <td>0.782082</td>\n      <td>0.068627</td>\n      <td>0.149292</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>[0.0009218235174879116, 0.9981522727362265, 0....</td>\n      <td>[0.021058089327376895, 0.8931636755248883, 0.0...</td>\n      <td>0.000922</td>\n      <td>0.998152</td>\n      <td>0.000926</td>\n      <td>0.021058</td>\n      <td>0.893164</td>\n      <td>0.085778</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.012857424035880197, 0.049620897662872476, 0...</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.012857</td>\n      <td>0.049621</td>\n      <td>0.937522</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>[0.0009218235174879116, 0.9981522727362265, 0....</td>\n      <td>[0.019859350988571122, 0.8760290135922674, 0.1...</td>\n      <td>0.000922</td>\n      <td>0.998152</td>\n      <td>0.000926</td>\n      <td>0.019859</td>\n      <td>0.876029</td>\n      <td>0.104112</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>[0.001415458355819997, 0.001409109992094977, 0...</td>\n      <td>[0.017046221848662425, 0.015254946699526208, 0...</td>\n      <td>0.001415</td>\n      <td>0.001409</td>\n      <td>0.997175</td>\n      <td>0.017046</td>\n      <td>0.015255</td>\n      <td>0.967699</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>[0.001415458355819997, 0.001409109992094977, 0...</td>\n      <td>[0.030917140205545455, 0.030766154069026847, 0...</td>\n      <td>0.001415</td>\n      <td>0.001409</td>\n      <td>0.997175</td>\n      <td>0.030917</td>\n      <td>0.030766</td>\n      <td>0.938317</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.10413216230659808, 0.08548414874560073, 0.8...</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.104132</td>\n      <td>0.085484</td>\n      <td>0.810384</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>[0.0009218235174879116, 0.9981522727362265, 0....</td>\n      <td>[0.00960348995787378, 0.9223250371009079, 0.06...</td>\n      <td>0.000922</td>\n      <td>0.998152</td>\n      <td>0.000926</td>\n      <td>0.009603</td>\n      <td>0.922325</td>\n      <td>0.068071</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.05880257190189563, 0.01195579734035659, 0.9...</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.058803</td>\n      <td>0.011956</td>\n      <td>0.929242</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>[0.001415458355819997, 0.001409109992094977, 0...</td>\n      <td>[0.013656152898532517, 0.013554892922842936, 0...</td>\n      <td>0.001415</td>\n      <td>0.001409</td>\n      <td>0.997175</td>\n      <td>0.013656</td>\n      <td>0.013555</td>\n      <td>0.972789</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>[0.9976707222247121, 0.0011622064773270835, 0....</td>\n      <td>[0.9732359394914631, 0.013369480667734494, 0.0...</td>\n      <td>0.997671</td>\n      <td>0.001162</td>\n      <td>0.001167</td>\n      <td>0.973236</td>\n      <td>0.013369</td>\n      <td>0.013395</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>[0.0009218235174879116, 0.9981522727362265, 0....</td>\n      <td>[0.020847029319461675, 0.9583195927092473, 0.0...</td>\n      <td>0.000922</td>\n      <td>0.998152</td>\n      <td>0.000926</td>\n      <td>0.020847</td>\n      <td>0.958320</td>\n      <td>0.020833</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.02566758798731632, 0.11389609959111871, 0.8...</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.025668</td>\n      <td>0.113896</td>\n      <td>0.860436</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>[0.0009218235174879116, 0.9981522727362265, 0....</td>\n      <td>[0.005677476709355365, 0.985942421661169, 0.00...</td>\n      <td>0.000922</td>\n      <td>0.998152</td>\n      <td>0.000926</td>\n      <td>0.005677</td>\n      <td>0.985942</td>\n      <td>0.008380</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>[0.9976707222247121, 0.0011622064773270835, 0....</td>\n      <td>[0.9140649906555004, 0.04637972020741349, 0.03...</td>\n      <td>0.997671</td>\n      <td>0.001162</td>\n      <td>0.001167</td>\n      <td>0.914065</td>\n      <td>0.046380</td>\n      <td>0.039555</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>[0.001415458355819997, 0.001409109992094977, 0...</td>\n      <td>[0.03733120591612821, 0.03126621596330079, 0.9...</td>\n      <td>0.001415</td>\n      <td>0.001409</td>\n      <td>0.997175</td>\n      <td>0.037331</td>\n      <td>0.031266</td>\n      <td>0.931403</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>[0.9976707222247121, 0.0011622064773270835, 0....</td>\n      <td>[0.9343074736777033, 0.031160849619190058, 0.0...</td>\n      <td>0.997671</td>\n      <td>0.001162</td>\n      <td>0.001167</td>\n      <td>0.934307</td>\n      <td>0.031161</td>\n      <td>0.034532</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>[0.001415458355819997, 0.001409109992094977, 0...</td>\n      <td>[0.020515611460293227, 0.021243669995712523, 0...</td>\n      <td>0.001415</td>\n      <td>0.001409</td>\n      <td>0.997175</td>\n      <td>0.020516</td>\n      <td>0.021244</td>\n      <td>0.958241</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>[0.001415458355819997, 0.001409109992094977, 0...</td>\n      <td>[0.031112080025803332, 0.030908036159298102, 0...</td>\n      <td>0.001415</td>\n      <td>0.001409</td>\n      <td>0.997175</td>\n      <td>0.031112</td>\n      <td>0.030908</td>\n      <td>0.937980</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>[0.0009218235174879116, 0.9981522727362265, 0....</td>\n      <td>[0.003753383273219095, 0.9924677984621932, 0.0...</td>\n      <td>0.000922</td>\n      <td>0.998152</td>\n      <td>0.000926</td>\n      <td>0.003753</td>\n      <td>0.992468</td>\n      <td>0.003779</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.03320876254696947, 0.009689440985234136, 0....</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.033209</td>\n      <td>0.009689</td>\n      <td>0.957102</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>[0.0009218235174879116, 0.9981522727362265, 0....</td>\n      <td>[0.01149989043393082, 0.9770042522121551, 0.01...</td>\n      <td>0.000922</td>\n      <td>0.998152</td>\n      <td>0.000926</td>\n      <td>0.011500</td>\n      <td>0.977004</td>\n      <td>0.011496</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.004978415178569605, 0.005081976104355061, 0...</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.004978</td>\n      <td>0.005082</td>\n      <td>0.989940</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.02879489001922393, 0.09745626123637435, 0.8...</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.028795</td>\n      <td>0.097456</td>\n      <td>0.873749</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>[0.001415458355819997, 0.001409109992094977, 0...</td>\n      <td>[0.03214614841115621, 0.008345504154948266, 0....</td>\n      <td>0.001415</td>\n      <td>0.001409</td>\n      <td>0.997175</td>\n      <td>0.032146</td>\n      <td>0.008346</td>\n      <td>0.959508</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>[0.0011688524885937544, 0.001174645211872204, ...</td>\n      <td>[0.011851729014614266, 0.02987654446203027, 0....</td>\n      <td>0.001169</td>\n      <td>0.001175</td>\n      <td>0.997657</td>\n      <td>0.011852</td>\n      <td>0.029877</td>\n      <td>0.958272</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'prompt_topics' and 'summary_topics' columns from object to float\n",
    "#result['prompt_topics'] = result['prompt_topics'].apply(lambda x: [float(val) for val in x])\n",
    "#result['summary_topics'] = result['summary_topics'].apply(lambda x: [float(val) for val in x])\n",
    "# Separate each item in 'prompt_topics' and 'summary_topics' into separate columns\n",
    "\n",
    "\n",
    "result[['prompt_topic_1', 'prompt_topic_2', 'prompt_topic_3']] = result['prompt_topics'].apply(pd.Series)\n",
    "result[['summary_topic_1', 'summary_topic_2', 'summary_topic_3']] = result['summary_topics'].apply(pd.Series)\n",
    "\n",
    "\n",
    "result[['prompt_question', 'prompt_title', 'prompt_topics', 'summary_topics', 'prompt_topic_1', 'prompt_topic_2', 'prompt_topic_3', 'summary_topic_1', 'summary_topic_2', 'summary_topic_3']].head(30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:24:42.799815Z",
     "start_time": "2023-10-11T00:24:42.129754400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7165 entries, 0 to 7164\n",
      "Data columns (total 66 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   student_id                    7165 non-null   object \n",
      " 1   prompt_id                     7165 non-null   object \n",
      " 2   text                          7165 non-null   object \n",
      " 3   content                       7165 non-null   float64\n",
      " 4   wording                       7165 non-null   float64\n",
      " 5   avg_sentence_length_summary   7165 non-null   float64\n",
      " 6   avg_word_length_summary       7165 non-null   float64\n",
      " 7   text_pos_tags                 7165 non-null   object \n",
      " 8   text_noun_count               7165 non-null   int64  \n",
      " 9   text_verb_count               7165 non-null   int64  \n",
      " 10  textblob_polarity             7165 non-null   float64\n",
      " 11  textblob_subjectivity         7165 non-null   float64\n",
      " 12  original_summary_len          7165 non-null   int64  \n",
      " 13  summary_sentenceCount         7165 non-null   int64  \n",
      " 14  summary_length                7165 non-null   int64  \n",
      " 15  summary_word_cnt              7165 non-null   int64  \n",
      " 16  fixed_summary_text            7165 non-null   object \n",
      " 17  splling_err_num               7165 non-null   int64  \n",
      " 18  prompt_question               7165 non-null   object \n",
      " 19  prompt_title                  7165 non-null   object \n",
      " 20  prompt_text                   7165 non-null   object \n",
      " 21  original_prompt_len           7165 non-null   int64  \n",
      " 22  prompt_sentenceCount          7165 non-null   int64  \n",
      " 23  prompt_length                 7165 non-null   int64  \n",
      " 24  prompt_word_cnt               7165 non-null   int64  \n",
      " 25  prompt_stpword_cnt            7165 non-null   int64  \n",
      " 26  question_pos_tags             7165 non-null   object \n",
      " 27  prompt_text_pos_tags          7165 non-null   object \n",
      " 28  noun_count                    7165 non-null   int64  \n",
      " 29  verb_count                    7165 non-null   int64  \n",
      " 30  prompt_textblob_polarity      7165 non-null   float64\n",
      " 31  prompt_textblob_subjectivity  7165 non-null   float64\n",
      " 32  prompt_keywords               7165 non-null   object \n",
      " 33  avg_sentence_length_prompt    7165 non-null   float64\n",
      " 34  avg_word_length_prompt        7165 non-null   float64\n",
      " 35  word_overlap_count            7165 non-null   int64  \n",
      " 36  bigram_overlap_count          7165 non-null   int64  \n",
      " 37  bigram_overlap_ratio          7165 non-null   float64\n",
      " 38  trigram_overlap_count         7165 non-null   int64  \n",
      " 39  trigram_overlap_ratio         7165 non-null   float64\n",
      " 40  quotes_count                  7165 non-null   int64  \n",
      " 41  keyword_count                 7165 non-null   int64  \n",
      " 42  text_pos_len                  7165 non-null   int64  \n",
      " 43  prompt_pos_len                7165 non-null   int64  \n",
      " 44  text_adj_count                7165 non-null   int64  \n",
      " 45  prompt_adj_count              7165 non-null   int64  \n",
      " 46  text_adj_ratio                7165 non-null   float64\n",
      " 47  prompt_adj_ration             7165 non-null   float64\n",
      " 48  text_verb_ratio               7165 non-null   float64\n",
      " 49  prompt_verb_ration            7165 non-null   float64\n",
      " 50  text_noun_ratio               7165 non-null   float64\n",
      " 51  prompt_noun_ration            7165 non-null   float64\n",
      " 52  flesch_kincaid_prompt         7165 non-null   float64\n",
      " 53  flesch_kincaid_summary        7165 non-null   float64\n",
      " 54  gunning_fog_prompt            7165 non-null   float64\n",
      " 55  gunning_fog_summary           7165 non-null   float64\n",
      " 56  jaccard_similarity            7165 non-null   float64\n",
      " 57  combined_text                 7165 non-null   object \n",
      " 58  prompt_topics                 7165 non-null   object \n",
      " 59  summary_topics                7165 non-null   object \n",
      " 60  prompt_topic_1                7165 non-null   float64\n",
      " 61  prompt_topic_2                7165 non-null   float64\n",
      " 62  prompt_topic_3                7165 non-null   float64\n",
      " 63  summary_topic_1               7165 non-null   float64\n",
      " 64  summary_topic_2               7165 non-null   float64\n",
      " 65  summary_topic_3               7165 non-null   float64\n",
      "dtypes: float64(29), int64(23), object(14)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "result.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:10.850207100Z",
     "start_time": "2023-10-11T00:15:10.834546600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "result = result.drop(['prompt_topics', 'summary_topics'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:10.897169200Z",
     "start_time": "2023-10-11T00:15:10.850207100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "result.describe()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:07.292062Z",
     "iopub.execute_input": "2023-09-24T14:17:07.292475Z",
     "iopub.status.idle": "2023-09-24T14:17:07.328292Z",
     "shell.execute_reply.started": "2023-09-24T14:17:07.292439Z",
     "shell.execute_reply": "2023-09-24T14:17:07.327368Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:10.944408600Z",
     "start_time": "2023-10-11T00:15:10.865836500Z"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "           content      wording  avg_sentence_length_summary  \\\ncount  7165.000000  7165.000000                  7165.000000   \nmean     -0.014853    -0.063072                    23.932378   \nstd       1.043569     1.036048                    14.268509   \nmin      -1.729859    -1.962614                     5.125000   \n25%      -0.799545    -0.872720                    15.000000   \n50%      -0.093814    -0.081769                    20.500000   \n75%       0.499660     0.503833                    28.800000   \nmax       3.900326     4.310693                   471.000000   \n\n       avg_word_length_summary  text_noun_count  text_verb_count  \\\ncount              7165.000000      7165.000000      7165.000000   \nmean                  4.563326        11.934822         3.998744   \nstd                   0.419488         8.758633         3.574231   \nmin                   3.258065         0.000000         0.000000   \n25%                   4.275862         6.000000         2.000000   \n50%                   4.545455         9.000000         3.000000   \n75%                   4.827586        15.000000         5.000000   \nmax                   7.015625       103.000000        32.000000   \n\n       textblob_polarity  textblob_subjectivity  original_summary_len  \\\ncount        7165.000000            7165.000000           7165.000000   \nmean            0.080354               0.437366            418.776971   \nstd             0.192219               0.181227            307.833685   \nmin            -1.000000               0.000000            114.000000   \n25%            -0.013310               0.333333            216.000000   \n50%             0.083333               0.450000            320.000000   \n75%             0.190625               0.551429            513.000000   \nmax             1.000000               1.000000           3940.000000   \n\n       summary_sentenceCount  ...  flesch_kincaid_summary  gunning_fog_prompt  \\\ncount            7165.000000  ...             7165.000000         7165.000000   \nmean                3.763015  ...                9.114613           11.309216   \nstd                 3.110060  ...                4.534061            3.583176   \nmin                 1.000000  ...                0.000000            6.560000   \n25%                 2.000000  ...                6.200000            6.560000   \n50%                 3.000000  ...                8.200000           10.270000   \n75%                 5.000000  ...               11.000000           14.430000   \nmax                47.000000  ...               61.000000           16.020000   \n\n       gunning_fog_summary  jaccard_similarity  prompt_topic_1  \\\ncount          7165.000000         7165.000000     7165.000000   \nmean             11.173788            0.089429        0.154575   \nstd               4.821147            0.054385        0.359656   \nmin               2.640000            0.000000        0.000922   \n25%               8.010000            0.052632        0.000922   \n50%              10.300000            0.079646        0.001169   \n75%              13.200000            0.113924        0.001415   \nmax              64.410000            0.481481        0.997671   \n\n       prompt_topic_2  prompt_topic_3  summary_topic_1  summary_topic_2  \\\ncount     7165.000000     7165.000000      7165.000000      7165.000000   \nmean         0.278974        0.566450         0.168779         0.282777   \nstd          0.446934        0.493677         0.315830         0.405843   \nmin          0.001162        0.000926         0.001464         0.001368   \n25%          0.001175        0.000926         0.013891         0.015500   \n50%          0.001409        0.997175         0.024229         0.028107   \n75%          0.998152        0.997657         0.074879         0.857096   \nmax          0.998152        0.997657         0.992219         0.995308   \n\n       summary_topic_3  \ncount      7165.000000  \nmean          0.548444  \nstd           0.441937  \nmin           0.002126  \n25%           0.030303  \n50%           0.866050  \n75%           0.957449  \nmax           0.997039  \n\n[8 rows x 52 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>wording</th>\n      <th>avg_sentence_length_summary</th>\n      <th>avg_word_length_summary</th>\n      <th>text_noun_count</th>\n      <th>text_verb_count</th>\n      <th>textblob_polarity</th>\n      <th>textblob_subjectivity</th>\n      <th>original_summary_len</th>\n      <th>summary_sentenceCount</th>\n      <th>...</th>\n      <th>flesch_kincaid_summary</th>\n      <th>gunning_fog_prompt</th>\n      <th>gunning_fog_summary</th>\n      <th>jaccard_similarity</th>\n      <th>prompt_topic_1</th>\n      <th>prompt_topic_2</th>\n      <th>prompt_topic_3</th>\n      <th>summary_topic_1</th>\n      <th>summary_topic_2</th>\n      <th>summary_topic_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>...</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n      <td>7165.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-0.014853</td>\n      <td>-0.063072</td>\n      <td>23.932378</td>\n      <td>4.563326</td>\n      <td>11.934822</td>\n      <td>3.998744</td>\n      <td>0.080354</td>\n      <td>0.437366</td>\n      <td>418.776971</td>\n      <td>3.763015</td>\n      <td>...</td>\n      <td>9.114613</td>\n      <td>11.309216</td>\n      <td>11.173788</td>\n      <td>0.089429</td>\n      <td>0.154575</td>\n      <td>0.278974</td>\n      <td>0.566450</td>\n      <td>0.168779</td>\n      <td>0.282777</td>\n      <td>0.548444</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.043569</td>\n      <td>1.036048</td>\n      <td>14.268509</td>\n      <td>0.419488</td>\n      <td>8.758633</td>\n      <td>3.574231</td>\n      <td>0.192219</td>\n      <td>0.181227</td>\n      <td>307.833685</td>\n      <td>3.110060</td>\n      <td>...</td>\n      <td>4.534061</td>\n      <td>3.583176</td>\n      <td>4.821147</td>\n      <td>0.054385</td>\n      <td>0.359656</td>\n      <td>0.446934</td>\n      <td>0.493677</td>\n      <td>0.315830</td>\n      <td>0.405843</td>\n      <td>0.441937</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.729859</td>\n      <td>-1.962614</td>\n      <td>5.125000</td>\n      <td>3.258065</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>114.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>6.560000</td>\n      <td>2.640000</td>\n      <td>0.000000</td>\n      <td>0.000922</td>\n      <td>0.001162</td>\n      <td>0.000926</td>\n      <td>0.001464</td>\n      <td>0.001368</td>\n      <td>0.002126</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.799545</td>\n      <td>-0.872720</td>\n      <td>15.000000</td>\n      <td>4.275862</td>\n      <td>6.000000</td>\n      <td>2.000000</td>\n      <td>-0.013310</td>\n      <td>0.333333</td>\n      <td>216.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>6.200000</td>\n      <td>6.560000</td>\n      <td>8.010000</td>\n      <td>0.052632</td>\n      <td>0.000922</td>\n      <td>0.001175</td>\n      <td>0.000926</td>\n      <td>0.013891</td>\n      <td>0.015500</td>\n      <td>0.030303</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.093814</td>\n      <td>-0.081769</td>\n      <td>20.500000</td>\n      <td>4.545455</td>\n      <td>9.000000</td>\n      <td>3.000000</td>\n      <td>0.083333</td>\n      <td>0.450000</td>\n      <td>320.000000</td>\n      <td>3.000000</td>\n      <td>...</td>\n      <td>8.200000</td>\n      <td>10.270000</td>\n      <td>10.300000</td>\n      <td>0.079646</td>\n      <td>0.001169</td>\n      <td>0.001409</td>\n      <td>0.997175</td>\n      <td>0.024229</td>\n      <td>0.028107</td>\n      <td>0.866050</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.499660</td>\n      <td>0.503833</td>\n      <td>28.800000</td>\n      <td>4.827586</td>\n      <td>15.000000</td>\n      <td>5.000000</td>\n      <td>0.190625</td>\n      <td>0.551429</td>\n      <td>513.000000</td>\n      <td>5.000000</td>\n      <td>...</td>\n      <td>11.000000</td>\n      <td>14.430000</td>\n      <td>13.200000</td>\n      <td>0.113924</td>\n      <td>0.001415</td>\n      <td>0.998152</td>\n      <td>0.997657</td>\n      <td>0.074879</td>\n      <td>0.857096</td>\n      <td>0.957449</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.900326</td>\n      <td>4.310693</td>\n      <td>471.000000</td>\n      <td>7.015625</td>\n      <td>103.000000</td>\n      <td>32.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3940.000000</td>\n      <td>47.000000</td>\n      <td>...</td>\n      <td>61.000000</td>\n      <td>16.020000</td>\n      <td>64.410000</td>\n      <td>0.481481</td>\n      <td>0.997671</td>\n      <td>0.998152</td>\n      <td>0.997657</td>\n      <td>0.992219</td>\n      <td>0.995308</td>\n      <td>0.997039</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 52 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:10.960036700Z",
     "start_time": "2023-10-11T00:15:10.944408600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract feature columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "FEATURE_COLUMNS = result.drop(columns = ['student_id', 'prompt_id', 'fixed_summary_text', 'text', 'prompt_question', 'prompt_title', 'prompt_text','content', 'wording', 'prompt_sentenceCount', 'summary_stpword_cnt','prompt_stpword_cnt', 'summary_punct_cnt', 'prompt_punct_cnt', 'summary_num_cnt', 'splling_err_num', 'prompt_num_cnt', 'quotes_count', 'question_pos_tags', 'text_pos_tags', 'prompt_text_pos_tags', 'prompt_keywords', 'combined_text'], axis = 1).columns.to_list()\n",
    "\n",
    "#columns = ['student_id', 'prompt_id', 'text', 'prompt_question', 'prompt_title', 'prompt_text', 'content', 'wording'], axis = 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:07.330314Z",
     "iopub.execute_input": "2023-09-24T14:17:07.330717Z",
     "iopub.status.idle": "2023-09-24T14:17:07.338152Z",
     "shell.execute_reply.started": "2023-09-24T14:17:07.330622Z",
     "shell.execute_reply": "2023-09-24T14:17:07.336711Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:12.223672Z",
     "start_time": "2023-10-11T00:15:10.960036700Z"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['summary_stpword_cnt', 'summary_punct_cnt', 'prompt_punct_cnt', 'summary_num_cnt', 'prompt_num_cnt'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m FEATURE_COLUMNS \u001B[38;5;241m=\u001B[39m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstudent_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfixed_summary_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_question\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_title\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwording\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_sentenceCount\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msummary_stpword_cnt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_stpword_cnt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msummary_punct_cnt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_punct_cnt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msummary_num_cnt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msplling_err_num\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_num_cnt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mquotes_count\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mquestion_pos_tags\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext_pos_tags\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_text_pos_tags\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt_keywords\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcombined_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mto_list()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#columns = ['student_id', 'prompt_id', 'text', 'prompt_question', 'prompt_title', 'prompt_text', 'content', 'wording'], axis = 1\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bowja\\lib\\site-packages\\pandas\\core\\frame.py:5347\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   5199\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[0;32m   5200\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   5201\u001B[0m     labels: IndexLabel \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5208\u001B[0m     errors: IgnoreRaise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   5209\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   5210\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   5211\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[0;32m   5212\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5345\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[0;32m   5346\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 5347\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   5348\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5349\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5350\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5351\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5352\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5353\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5354\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5355\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bowja\\lib\\site-packages\\pandas\\core\\generic.py:4711\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   4709\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   4710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 4711\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4713\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[0;32m   4714\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bowja\\lib\\site-packages\\pandas\\core\\generic.py:4753\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[1;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[0;32m   4751\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m   4752\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 4753\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m \u001B[43maxis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4754\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[0;32m   4756\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[0;32m   4757\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bowja\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6992\u001B[0m, in \u001B[0;36mIndex.drop\u001B[1;34m(self, labels, errors)\u001B[0m\n\u001B[0;32m   6990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[0;32m   6991\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 6992\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels[mask]\u001B[38;5;241m.\u001B[39mtolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   6993\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[0;32m   6994\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['summary_stpword_cnt', 'summary_punct_cnt', 'prompt_punct_cnt', 'summary_num_cnt', 'prompt_num_cnt'] not found in axis\""
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "FEATURE_COLUMNS"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:07.339466Z",
     "iopub.execute_input": "2023-09-24T14:17:07.339803Z",
     "iopub.status.idle": "2023-09-24T14:17:07.362857Z",
     "shell.execute_reply.started": "2023-09-24T14:17:07.339775Z",
     "shell.execute_reply": "2023-09-24T14:17:07.361591Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:12.223672Z",
     "start_time": "2023-10-11T00:15:12.223672Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:12.229888700Z",
     "start_time": "2023-10-11T00:15:12.223672Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot feature columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# figure, axis = plt.subplots(3, 2, figsize=(15, 15))\n",
    "# plt.subplots_adjust(hspace=0.25, wspace=0.3)\n",
    "\n",
    "# for i, column_name in enumerate(FEATURE_COLUMNS):\n",
    "#     row = i//2\n",
    "#     col = i % 2\n",
    "#     bp = sns.barplot(ax=axis[row, col], x=preprocessed_df['student_id'], y=preprocessed_df[column_name], color='blue')\n",
    "#     bp.set(xticklabels=[])\n",
    "#     axis[row, col].set_title(column_name)\n",
    "# axis[2, 1].set_visible(False)\n",
    "# plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:07.579177Z",
     "iopub.execute_input": "2023-09-24T14:17:07.579525Z",
     "iopub.status.idle": "2023-09-24T14:17:07.583711Z",
     "shell.execute_reply.started": "2023-09-24T14:17:07.579497Z",
     "shell.execute_reply": "2023-09-24T14:17:07.582561Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:12.229888700Z",
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us split the dataset into training and testing datasets:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "def split_dataset(dataset, test_ratio=0.20):\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]\n",
    "\n",
    "train_ds_pd, valid_ds_pd = split_dataset(result)\n",
    "train_ds_pd.shape, valid_ds_pd.shape\n",
    "\"\"\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:08.349160Z",
     "iopub.execute_input": "2023-09-24T14:17:08.349549Z",
     "iopub.status.idle": "2023-09-24T14:17:08.364119Z",
     "shell.execute_reply.started": "2023-09-24T14:17:08.349516Z",
     "shell.execute_reply": "2023-09-24T14:17:08.362951Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# `content` label datatset columns\n",
    "FEATURE_CONTENT = FEATURE_COLUMNS\n",
    "\n",
    "# `wording` label datatset columns\n",
    "FEATURE_WORDING = FEATURE_COLUMNS"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:09.055338Z",
     "iopub.execute_input": "2023-09-24T14:17:09.055704Z",
     "iopub.status.idle": "2023-09-24T14:17:09.061398Z",
     "shell.execute_reply.started": "2023-09-24T14:17:09.055672Z",
     "shell.execute_reply": "2023-09-24T14:17:09.060168Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'n_estimators': 70,  # Adjust as needed\n",
    "    'max_depth': 5,       # Adjust as needed\n",
    "    'eta': 0.1, \n",
    "    'subsample' : 0.5 ,\n",
    "    'colsample_bytree' : 0.7# Add other hyperparameters here\n",
    "}\n",
    "\n",
    "# Create RandomForestModel for label content\n",
    "model_content = XGBRegressor(**params)\n",
    "\n",
    "\n",
    "# Create RandomForestModel for label wording\n",
    "model_wording = XGBRegressor(**params)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:10.022257Z",
     "iopub.execute_input": "2023-09-24T14:17:10.022687Z",
     "iopub.status.idle": "2023-09-24T14:17:10.028909Z",
     "shell.execute_reply.started": "2023-09-24T14:17:10.022654Z",
     "shell.execute_reply": "2023-09-24T14:17:10.027402Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_splits = 4  # Choose the number of folds you want\n",
    "group_kfold = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "\n",
    "scores_c = []  # To store the evaluation scores for each fold\n",
    "scores_w = []\n",
    "\n",
    "for train_idx, test_idx in group_kfold.split(result[FEATURE_COLUMNS],  groups= result['prompt_id']):\n",
    "    #X_train, X_test = result[FEATURE_COLUMNS][train_idx], result[FEATURE_COLUMNS][test_idx]\n",
    "    X_train = result.loc[train_idx, FEATURE_COLUMNS]\n",
    "    X_test = result.loc[test_idx, FEATURE_COLUMNS]\n",
    "\n",
    "    y_train_c, y_test_c = result['content'][train_idx], result['content'][test_idx]\n",
    "    y_train_w, y_test_w = result['wording'][train_idx], result['wording'][test_idx]\n",
    "    \n",
    "    # Fit the XGBoost regressor on the training data\n",
    "    # Training RandomForestModel for label content\n",
    "    model_content.fit(X_train, y_train_c)\n",
    "\n",
    "    # Training RandomForestModel for label wording\n",
    "    model_wording.fit(X_train, y_train_w)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    evaluation_content = model_content.predict(X_test)\n",
    "    evaluation_content_rmse = np.sqrt(np.mean((evaluation_content - y_test_c)**2))\n",
    "    scores_c.append(evaluation_content_rmse)\n",
    "    \n",
    "    evaluation_wording = model_wording.predict(X_test)\n",
    "    evaluation_wording_rmse = np.sqrt(np.mean((evaluation_wording - y_test_w)**2))\n",
    "    scores_w.append(evaluation_wording_rmse)\n",
    "\n",
    "# Calculate the average score across all folds\n",
    "average_score_c = np.mean(scores_c)\n",
    "print(f'Average Root Mean Squared Error Content: {average_score_c}')\n",
    "\n",
    "# Calculate the average score across all folds\n",
    "average_score_w = np.mean(scores_w)\n",
    "print(f'Average Root Mean Squared Error Wording: {average_score_w}')\n",
    "\n",
    "MCRMSE = np.mean([average_score_c, average_score_w])\n",
    "print(f\"MCRMSE: {MCRMSE:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model\n",
    "\n",
    "We will train the model using a one-liner.\n",
    "\n",
    "Note: you may see a warning about Autograph. You can safely ignore this, it will be fixed in the next release."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Training RandomForestModel for label content\n",
    "#model_content.fit(train_ds_pd[FEATURE_CONTENT], train_ds_pd['content'])\n",
    "\n",
    "# Training RandomForestModel for label wording\n",
    "#model_wording.fit(train_ds_pd[FEATURE_WORDING], train_ds_pd['wording'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:10.396636Z",
     "iopub.execute_input": "2023-09-24T14:17:10.396966Z",
     "iopub.status.idle": "2023-09-24T14:17:11.693215Z",
     "shell.execute_reply.started": "2023-09-24T14:17:10.396939Z",
     "shell.execute_reply": "2023-09-24T14:17:11.692561Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us run an evaluation using the validation dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#evaluation_content = model_content.predict(valid_ds_pd[FEATURE_CONTENT])\n",
    "#evaluation_content_rmse = np.sqrt(np.mean((evaluation_content - valid_ds_pd['content'])**2))\n",
    "# evaluation_content = model_content.score(valid_ds_pd[FEATURE_CONTENT], valid_ds_pd['content'])\n",
    "#print(f\"Content MSE: {evaluation_content_rmse:.4f}\")\n",
    "\n",
    "# Run evaluation for model_wording\n",
    "#evaluation_wording = model_wording.predict(valid_ds_pd[FEATURE_WORDING])\n",
    "#evaluation_wording_rmse = np.sqrt(np.mean((evaluation_wording - valid_ds_pd['wording'])**2))\n",
    "#print(f\"Wording MSE: {evaluation_wording_rmse:.4f}\")\n",
    "\n",
    "#MCRMSE = np.mean([evaluation_content_rmse, evaluation_wording_rmse])\n",
    "#print(f\"MCRMSE: {MCRMSE:.4f}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:11.694397Z",
     "iopub.execute_input": "2023-09-24T14:17:11.694661Z",
     "iopub.status.idle": "2023-09-24T14:17:11.735187Z",
     "shell.execute_reply.started": "2023-09-24T14:17:11.694637Z",
     "shell.execute_reply": "2023-09-24T14:17:11.734093Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#df_test_prompt = pd.read_csv('data/prompts_test.csv')\n",
    "#df_test_summaries = pd.read_csv('data/summaries_test.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:11.889387Z",
     "iopub.execute_input": "2023-09-24T14:17:11.889693Z",
     "iopub.status.idle": "2023-09-24T14:17:11.899589Z",
     "shell.execute_reply.started": "2023-09-24T14:17:11.889666Z",
     "shell.execute_reply": "2023-09-24T14:17:11.898545Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:12.229888700Z",
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#df_test = df_test_summaries.merge(df_test_prompt, on='prompt_id')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:12.292124Z",
     "iopub.execute_input": "2023-09-24T14:17:12.292836Z",
     "iopub.status.idle": "2023-09-24T14:17:12.301327Z",
     "shell.execute_reply.started": "2023-09-24T14:17:12.292805Z",
     "shell.execute_reply": "2023-09-24T14:17:12.299840Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.229888700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#df_test.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:12.559125Z",
     "iopub.execute_input": "2023-09-24T14:17:12.560236Z",
     "iopub.status.idle": "2023-09-24T14:17:12.572472Z",
     "shell.execute_reply.started": "2023-09-24T14:17:12.560200Z",
     "shell.execute_reply": "2023-09-24T14:17:12.571603Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:12.243503600Z",
     "start_time": "2023-10-11T00:15:12.243503600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#processed_test_df = feature_engineer(df_test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:13.103841Z",
     "iopub.execute_input": "2023-09-24T14:17:13.104222Z",
     "iopub.status.idle": "2023-09-24T14:17:13.114568Z",
     "shell.execute_reply.started": "2023-09-24T14:17:13.104190Z",
     "shell.execute_reply": "2023-09-24T14:17:13.113475Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.243503600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#processed_test_df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:13.387752Z",
     "iopub.execute_input": "2023-09-24T14:17:13.388115Z",
     "iopub.status.idle": "2023-09-24T14:17:13.401569Z",
     "shell.execute_reply.started": "2023-09-24T14:17:13.388088Z",
     "shell.execute_reply": "2023-09-24T14:17:13.400612Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T00:15:12.243503600Z",
     "start_time": "2023-10-11T00:15:12.243503600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(processed_test_df[FEATURE_COLUMNS], task = tfdf.keras.Task.REGRESSION)\n",
    "#test_ds = processed_test_df[FEATURE_COLUMNS]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:13.590827Z",
     "iopub.execute_input": "2023-09-24T14:17:13.591383Z",
     "iopub.status.idle": "2023-09-24T14:17:13.597749Z",
     "shell.execute_reply.started": "2023-09-24T14:17:13.591347Z",
     "shell.execute_reply": "2023-09-24T14:17:13.596273Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.243503600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#processed_test_df['content'] = model_content.predict(test_ds)\n",
    "#processed_test_df['wording'] = model_wording.predict(test_ds)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:14.035736Z",
     "iopub.execute_input": "2023-09-24T14:17:14.036112Z",
     "iopub.status.idle": "2023-09-24T14:17:14.055044Z",
     "shell.execute_reply.started": "2023-09-24T14:17:14.036079Z",
     "shell.execute_reply": "2023-09-24T14:17:14.053762Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.243503600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#processed_test_df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:14.277892Z",
     "iopub.execute_input": "2023-09-24T14:17:14.278230Z",
     "iopub.status.idle": "2023-09-24T14:17:14.293059Z",
     "shell.execute_reply.started": "2023-09-24T14:17:14.278202Z",
     "shell.execute_reply": "2023-09-24T14:17:14.292004Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.243503600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#processed_test_df[['student_id', 'content', 'wording']].to_csv('submission.csv',index=False)\n",
    "# display(pd.read_csv('submission.csv'))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-24T14:17:14.816477Z",
     "iopub.execute_input": "2023-09-24T14:17:14.816983Z",
     "iopub.status.idle": "2023-09-24T14:17:14.829164Z",
     "shell.execute_reply.started": "2023-09-24T14:17:14.816952Z",
     "shell.execute_reply": "2023-09-24T14:17:14.828389Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.243503600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-11T00:15:12.243503600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
